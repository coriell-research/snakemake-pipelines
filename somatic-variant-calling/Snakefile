import pandas as pd


configfile: "config.yaml"

WORK_DIR = "../data"
FASTP_OUT = f"{WORK_DIR}/fastp"
BWA_OUT = f"{WORK_DIR}/bwa"
MERGE_OUT = f"{WORK_DIR}/merge"
MARKDUP_OUT = f"{WORK_DIR}/markdup"
VARSCAN_PAIRED_OUT = f"{WORK_DIR}/varscan_paired"
MUSE_PAIRED_OUT = f"{WORK_DIR}/muse_paired"
PILEUP_SUMMARY_OUT = f"{WORK_DIR}/pileup_summary"
MUTECT_PAIRED_OUT = f"{WORK_DIR}/mutect_paired"
CONTAM_PAIRED_OUT = f"{WORK_DIR}/contam_paired"
CONSENSUS_PAIRED_OUT = f"{WORK_DIR}/isec_paired"
ANNOTATED_PAIRED_OUT = f"{WORK_DIR}/annotate_paired"

# Define samples and experimental units
SAMPLES = pd.read_csv("samples.csv", sep=",", header=0)
UNITS = SAMPLES["PU"].tolist()

# Extract the PUs that need to be merged into single sample BAMs
PU_BY_SAMPLE = SAMPLES.groupby('SM')['PU'].agg(list).to_dict()

def get_replicates_for_merge(wildcards):
    units = PU_BY_SAMPLE[wildcards.sample]
    return [f"{BWA_OUT}/{u}.sorted.bam" for u in units]


# Extract a dictionary of tumor-normal pairs and single samples
PAIRED_SAMPLES = {}
SINGLE_SAMPLES = {}
for sample_name, group in SAMPLES.groupby("sample_name"):
    sample_id = str(sample_name)
    tumor_sms = group[group["tumor_normal"] == "tumor"]["SM"].unique()
    normal_sms = group[group["tumor_normal"] == "normal"]["SM"].unique()
    
    if len(tumor_sms) > 0:
        tumor_sm = tumor_sms[0]
        if len(normal_sms) > 0:
            normal_sm = normal_sms[0]
            PAIRED_SAMPLES[sample_id] = {"tumor": tumor_sm, "normal": normal_sm}
        else:
            SINGLE_SAMPLES[sample_id] = {"tumor": tumor_sm}


# Rules ----------------------------------------------------------------------


rule all:
    input:
        expand(f"{ANNOTATED_PAIRED_OUT}/{{sample}}.maf", sample=PAIRED_SAMPLES.keys())


rule fastp:
    input:
        r1 = lambda w: SAMPLES.loc[SAMPLES["PU"] == w.pu, "read1"].item(),
        r2 = lambda w: SAMPLES.loc[SAMPLES["PU"] == w.pu, "read2"].item()
    output:
        r1_out = temp(f"{FASTP_OUT}/{{pu}}.trimmed.1.fq.gz"),
        r2_out = temp(f"{FASTP_OUT}/{{pu}}.trimmed.2.fq.gz"),
        html = f"{FASTP_OUT}/{{pu}}.fastp.html",
        json = f"{FASTP_OUT}/{{pu}}.fastp.json"
    log:
        f"{FASTP_OUT}/{{pu}}_fastp.log"
    params:
        extra = config["fastp"]["extra_params"]
    shell:
        "fastp -i {input.r1} -I {input.r2} -o {output.r1_out} -O {output.r2_out} "
        "{params.extra} --html {output.html} --json {output.json} &> {log}"


rule bwa:
    input:
        r1 = f"{FASTP_OUT}/{{pu}}.trimmed.1.fq.gz",
        r2 = f"{FASTP_OUT}/{{pu}}.trimmed.2.fq.gz",
    output:
        bam = f"{BWA_OUT}/{{pu}}.sorted.bam",
        bai = f"{BWA_OUT}/{{pu}}.sorted.bam.bai"
    params:
         pid = lambda w: SAMPLES.loc[SAMPLES["PU"] == w.pu, "ID"].item(),
         sm = lambda w: SAMPLES.loc[SAMPLES["PU"] == w.pu, "SM"].item(),
         pl = lambda w: SAMPLES.loc[SAMPLES["PU"] == w.pu, "PL"].item(),
         lb = lambda w: SAMPLES.loc[SAMPLES["PU"] == w.pu, "LB"].item(),
         pu = lambda w: SAMPLES.loc[SAMPLES["PU"] == w.pu, "PU"].item(),
         idx = config["bwa"]["idx"],
         gb = 4
    threads:
        8
    log:
        f"{BWA_OUT}/{{pu}}.log"
    shell:
        """
        bwa mem -t {threads} -R '@RG\\tID:{params.pid}\\tSM:{params.sm}\\tPL:{params.pl}\\tLB:{params.lb}\\tPU:{params.pu}' {params.idx} {input.r1} {input.r2} | \
        samtools sort -@{threads} -m{params.gb}g -o {output.bam} - && samtools index {output.bam}
        """


rule merge_bam:
    input:
        get_replicates_for_merge
    output:
        f"{MERGE_OUT}/{{sample}}.bam"
    params:
        bams = lambda w, input: " ".join([f"-I {f}" for f in input]),
        java_args = config["java"]["java_args"]
    log:
        f"{MERGE_OUT}/{{sample}}.log"
    shell:
        "java {params.java_args} -jar /usr/local/programs/picard.jar MergeSamFiles "
          "{params} "
          "--OUTPUT {output} "
          "--ASSUME_SORTED true "
          "--CREATE_INDEX true "
          "--SORT_ORDER coordinate"


rule mark_duplicates:
    input:
        f"{MERGE_OUT}/{{sample}}.bam"
    output:
        bam = f"{MARKDUP_OUT}/{{sample}}.bam",
        metrics = f"{MARKDUP_OUT}/{{sample}}.metrics.txt"
    params:
        java_args = config["java"]["java_args"]
    log:
        f"{MARKDUP_OUT}/{{sample}}.log",
    shell:
        "java {params.java_args} -jar /usr/local/programs/picard.jar MarkDuplicates "
          "-I {input} "
          "-O {output.bam} "
          "-M {output.metrics} "
          "--ASSUME_SORT_ORDER coordinate "
          "--CREATE_INDEX true "


rule varscan_paired:
    input:
        tumor = lambda w: f"{MARKDUP_OUT}/{PAIRED_SAMPLES[str(w.sample)]['tumor']}.bam",
        normal = lambda w: f"{MARKDUP_OUT}/{PAIRED_SAMPLES[str(w.sample)]['normal']}.bam"
    output:
        snp_vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.vcf",
        indel_vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.indel.vcf"
    log:
        f"{VARSCAN_PAIRED_OUT}/{{sample}}.somatic.log"
    params:
        max_depth = 100000,
        min_mapq = 20,
        ref = config["bwa"]["idx"],
        out_prefix = f"{VARSCAN_PAIRED_OUT}/{{sample}}",
        min_cov = 10,
        min_var_freq = 0.05,
        somatic_p = 0.05,
        strand_filter = 1
    shell:
        """
        samtools mpileup \
            -d {params.max_depth} \
            -q {params.min_mapq} \
            --no-BAQ \
            -f {params.ref} \
            {input.normal} {input.tumor} |
        java -jar /usr/local/programs/VarScan.jar somatic \
            - \
            {params.out_prefix} \
            --mpileup 1 \
            --output-vcf 1 \
            --min-coverage {params.min_cov} \
            --min-var-freq {params.min_var_freq} \
            --somatic-p-value {params.somatic_p} \
            --strand-filter {params.strand_filter} \
            --output-snp {output.snp_vcf} \
            --output-indel {output.indel_vcf} \
            2> {log}
        """


rule varscan_filter_paired:
    input:
        snp_vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.vcf",
        indel_vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.indel.vcf",
    output:
        vcf_filtered = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.vcf"
    log:
        f"{VARSCAN_PAIRED_OUT}/{{sample}}.filter.log"
    params:
        min_cov = 10,
        min_reads = 4,
        min_strands = 1,
        min_var_freq = 0.05,
        p_value = 0.05
    shell:
        """
        java -jar /usr/local/programs/VarScan.jar somaticFilter \
            {input.snp_vcf} \
            --min-coverage {params.min_cov} \
            --min-reads2 {params.min_reads} \
            --min-strands2 {params.min_strands} \
            --min-var-freq {params.min_var_freq} \
            --p-value {params.p_value} \
            --indel-file {input.indel_vcf} \
            --output-file {output.vcf_filtered} \
            2> {log}
        """

# Extracts only SNV positions from VarScan somatic hc calls for use in bam-readcounts
rule get_site_list:
    input:
        vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.vcf"
    output:
        site_list = f"{VARSCAN_PAIRED_OUT}/{{sample}}.sitelist"
    shell:
        """
        awk 'BEGIN{{FS=OFS="\t"}} !/^#/ {{print $1, $2, $2}}' {input.vcf} > {output.site_list}
        """

# Used for VarScan fpfilter
rule bam_readcounts:
    input:
        tumor_bam = lambda w: f"{MARKDUP_OUT}/{PAIRED_SAMPLES[str(w.sample)]['tumor']}.bam",
        vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.vcf",
        site_list = f"{VARSCAN_PAIRED_OUT}/{{sample}}.sitelist"
    output:
        readcounts = f"{VARSCAN_PAIRED_OUT}/{{sample}}.readcounts"
    log:
        f"{VARSCAN_PAIRED_OUT}/{{sample}}.readcounts.log"
    params:
        ref = config["bwa"]["idx"],
        max_depth = 100000,
        min_mapq = 20
    conda:
        "bam-readcount"
    shell:
        """
        bam-readcount \
          --min-mapping-quality {params.min_mapq} \
          --max-count {params.max_depth} \
          --reference-fasta {params.ref} \
          --site-list {input.site_list} \
          {input.tumor_bam} > {output.readcounts}
        """


rule varscan_fpfilter_paired:
    input:
        readcounts = f"{VARSCAN_PAIRED_OUT}/{{sample}}.readcounts",
        vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.vcf"
    output:
        vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.vcf"
    log:
        f"{VARSCAN_PAIRED_OUT}/{{sample}}.fpfilter.log"
    params:
        min_var_count = 3,
        min_var_freq  = 0.05,
        min_read_pos  = 0.10,
        min_strandedness = 0
    shell:
        """
        java -jar /usr/local/programs/VarScan.jar fpfilter \
            {input.vcf} \
            {input.readcounts} \
            --output-file {output.vcf} \
            --min-var-count {params.min_var_count} \
            --min-var-freq {params.min_var_freq} \
            --min-read-pos {params.min_read_pos} \
            --min-strandedness {params.min_strandedness} \
            2> {log}
        """

# Needed because VarScan does not add contigs to header
rule varscan_rehead_vcf:
    input:
        vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.vcf"
    output:
        vcf_gz = temp(f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.vcf.gz"),
        vcf_tbi = temp(f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.vcf.gz.tbi"),
        rehead = temp(f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.rehead.vcf.gz"),
        rehead_tbi = temp(f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.rehead.vcf.gz.tbi")
    params:
        fai = config["bwa"]["fai"]
    shell:
        """
        bcftools view -Oz -o {output.vcf_gz} {input.vcf} && bcftools index -t {output.vcf_gz} 
        bcftools reheader -f {params.fai} -o {output.rehead} {output.vcf_gz} && \
        bcftools index -t {output.rehead}
        """


rule varscan_pass_norm_paired:
    input:
        vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.rehead.vcf.gz",
        vcf_tbi = f"{VARSCAN_PAIRED_OUT}/{{sample}}.snp.Somatic.hc.fpfilter.rehead.vcf.gz.tbi",
    output:
        vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.pass.vcf.gz",
        tbi = f"{VARSCAN_PAIRED_OUT}/{{sample}}.pass.vcf.gz.tbi"
    params:
        ref = config["bwa"]["idx"],
        buff = 10000,
        multi_allele = "-",
        check_ref = "s"
    shell:
        """
        bcftools view --types 'snps' -f 'PASS' {input.vcf} | \
        bcftools norm -c {params.check_ref} -m {params.multi_allele} -w {params.buff} -f {params.ref} | \
        bcftools view -Oz -o {output.vcf} && \
        bcftools index -t {output.vcf}
        """


rule muse_call_paired:
    input:
        tumor = lambda w: f"{MARKDUP_OUT}/{PAIRED_SAMPLES[str(w.sample)]['tumor']}.bam",
        normal = lambda w: f"{MARKDUP_OUT}/{PAIRED_SAMPLES[str(w.sample)]['normal']}.bam"
    output:
        txt = f"{MUSE_PAIRED_OUT}/{{sample}}.MuSE.txt"
    log:
        f"{MUSE_PAIRED_OUT}/{{sample}}.call.log"
    params:
        ref = config["bwa"]["idx"],
        out_prefix = f"{MUSE_PAIRED_OUT}/{{sample}}"
    threads: 
        8
    conda:
        "muse"
    shell:
        """
        MuSE call \
            -O {params.out_prefix} \
            -f {params.ref} \
            -n {threads} \
            {input.tumor} {input.normal} \
            2> {log}
        """


rule muse_sump_paired:
    input:
        txt = f"{MUSE_PAIRED_OUT}/{{sample}}.MuSE.txt"
    output:
        vcf = f"{MUSE_PAIRED_OUT}/{{sample}}.vcf"
    log:
        f"{MUSE_PAIRED_OUT}/{{sample}}.sump.log"
    params:
        dbsnp = config["muse"]["dbsnp"],
        mode = "-E"
    threads:
        8
    conda:
        "muse"
    shell:
        """
        MuSE sump \
            -I {input.txt} \
            {params.mode} \
            -n {threads} \
            -O {output.vcf} \
            -D {params.dbsnp} \
            2> {log}
        """


rule muse_pass_norm_paired:
    input:
        vcf = f"{MUSE_PAIRED_OUT}/{{sample}}.vcf"
    output:
        vcf = f"{MUSE_PAIRED_OUT}/{{sample}}.pass.vcf.gz",
        tbi = f"{MUSE_PAIRED_OUT}/{{sample}}.pass.vcf.gz.tbi"
    params:
        ref = config["bwa"]["idx"],
        buff = 10000,
        multi_allele = "-",
        check_ref = "s"
    shell:
        """
        bcftools view --types 'snps' -f 'PASS' {input.vcf} | \
        bcftools norm -c {params.check_ref} -m {params.multi_allele} -w {params.buff} -f {params.ref} | \
        bcftools view -Oz -o {output.vcf} && \
        bcftools index -t {output.vcf}
        """


rule mutect2_paired:
    input:
        tumor = lambda w: f"{MARKDUP_OUT}/{PAIRED_SAMPLES[str(w.sample)]['tumor']}.bam",
        normal = lambda w: f"{MARKDUP_OUT}/{PAIRED_SAMPLES[str(w.sample)]['normal']}.bam"
    output:
        vcf = f"{MUTECT_PAIRED_OUT}/{{sample}}.vcf.gz",
        f1r2 = f"{MUTECT_PAIRED_OUT}/{{sample}}.f1r2.tar.gz"
    params:
        normal_name = lambda w: PAIRED_SAMPLES[str(w.sample)]['normal'],
        ref = config["gatk"]["ref"],
        glr = config["gatk"]["glr"],
        pon = config["gatk"]["pon"],
        sec = config["mutect"]["progress_seconds"],
        java_args = config["java"]["java_args"],
        tmp_dir = MUTECT_PAIRED_OUT
    log:
        f"{MUTECT_PAIRED_OUT}/{{sample}}.log"
    threads:
        4
    shell:
        "gatk --java-options {params.java_args} Mutect2 "
          "-I {input.tumor} "
          "-I {input.normal} "
          "-O {output.vcf} "
          "-R {params.ref} "
          "--tmp-dir {params.tmp_dir} "
          "--normal {params.normal_name} "
          "--germline-resource {params.glr} "
          "--panel-of-normals {params.pon} "
          "--f1r2-tar-gz {output.f1r2} "
          "--seconds-between-progress-updates {params.sec} "
          "--native-pair-hmm-threads {threads} "
          "--create-output-variant-index true  &> {log}"


rule pileup_summary:
    input:
        f"{MARKDUP_OUT}/{{sample}}.bam"
    output:
        f"{PILEUP_SUMMARY_OUT}/{{sample}}.table"
    params:
        common_snps = config["gatk"]["snps"],
        ref = config["gatk"]["ref"],
        sec = 120,
        java_args = config["java"]["java_args"],
        tmp_dir = PILEUP_SUMMARY_OUT
    log:
        f"{PILEUP_SUMMARY_OUT}/{{sample}}.pileupsummary.log"
    shell:
        "gatk --java-options {params.java_args} GetPileupSummaries "
          "-I {input} "
          "-O {output} "
          "-R {params.ref} "
          "-V {params.common_snps} "
          "-L {params.common_snps} "
          "--tmp-dir {params.tmp_dir} "
          "--seconds-between-progress-updates {params.sec} &> {log}"


rule contamination_paired:
    input:
        tumor = lambda w: f"{PILEUP_SUMMARY_OUT}/{PAIRED_SAMPLES[str(w.sample)]['tumor']}.table",
        normal = lambda w: f"{PILEUP_SUMMARY_OUT}/{PAIRED_SAMPLES[str(w.sample)]['normal']}.table"
    output:
        f"{CONTAM_PAIRED_OUT}/{{sample}}.table"
    params:
        java_args = config["java"]["java_args"],
        tmp_dir = CONTAM_PAIRED_OUT
    log:
        f"{CONTAM_PAIRED_OUT}/{{sample}}.log"
    shell:
        """
        gatk --java-options {params.java_args} CalculateContamination \
          --input {input.tumor} \
          --matched-normal {input.normal} \
          --output {output} \
          --tmp-dir {params.tmp_dir} &> {log}
        """


rule filter_mutect_paired:
    input:
        vcf = f"{MUTECT_PAIRED_OUT}/{{sample}}.vcf.gz",
        bias = f"{MUTECT_PAIRED_OUT}/{{sample}}.f1r2.tar.gz",
        contam = f"{CONTAM_PAIRED_OUT}/{{sample}}.table"
    output:
        vcf = f"{MUTECT_PAIRED_OUT}/{{sample}}.filtered.vcf"
    params:
        ref = config["gatk"]['ref'],
        strategy = config["mutect"]['strategy'],
        fdr = config["mutect"]['fdr'],
        alt_reads = config["mutect"]['alt_reads'],
        read_pos = config["mutect"]['read_pos'],
        base_qual = config["mutect"]['base_qual'],
        min_allele = config["mutect"]['min_allele'],
        tmp_dir = MUTECT_PAIRED_OUT,
        java_args = config["java"]["java_args"]
    log:
        f"{MUTECT_PAIRED_OUT}/{{sample}}.log"
    shell:
        "gatk --java-options {params.java_args} FilterMutectCalls "
          "-R {params.ref} "
          "-V {input.vcf} "
          "-O {output.vcf} "
          "--threshold-strategy {params.strategy} " 
          "--false-discovery-rate {params.fdr} "
          "--unique-alt-read-count {params.alt_reads} " 
          "--min-median-read-position {params.read_pos} " 
          "--min-median-base-quality {params.base_qual} "
          "--min-allele-fraction {params.min_allele} "
          "--contamination-table {input.contam} "
          "--tmp-dir {params.tmp_dir} &> {log}"


rule mutect_pass_norm_paired:
    input:
        vcf = f"{MUTECT_PAIRED_OUT}/{{sample}}.filtered.vcf"
    output:
        vcf = f"{MUTECT_PAIRED_OUT}/{{sample}}.pass.vcf.gz",
        tbi = f"{MUTECT_PAIRED_OUT}/{{sample}}.pass.vcf.gz.tbi"
    params:
        ref = config["bwa"]["idx"],
        buff = 10000,
        multi_allele = "-",
        check_ref = "s"
    shell:
        """
        bcftools view --types 'snps' -f 'PASS' {input.vcf} | \
        bcftools norm -c {params.check_ref} -m {params.multi_allele} -w {params.buff} -f {params.ref} | \
        bcftools view -Oz -o {output.vcf} && \
        bcftools index -t {output.vcf}
        """


rule consensus_calls:
    input:
        varscan_vcf = f"{VARSCAN_PAIRED_OUT}/{{sample}}.pass.vcf.gz",
        muse_vcf = f"{MUSE_PAIRED_OUT}/{{sample}}.pass.vcf.gz",
        mutect_vcf = f"{MUTECT_PAIRED_OUT}/{{sample}}.pass.vcf.gz"
    output:
        sites_vcf = f"{CONSENSUS_PAIRED_OUT}/{{sample}}/{{sample}}.vcf.gz"
    params:
        prefix_dir = f"{CONSENSUS_PAIRED_OUT}/{{sample}}",
        ref = config["bwa"]["idx"]
    shell:
        """
        # Get the consensus
        bcftools isec -c all -n=3 {input.varscan_vcf} {input.muse_vcf} {input.mutect_vcf} -p {params.prefix_dir}
        
        # Create a regions file to use for subsetting
        awk 'BEGIN {{OFS="\t"}} {{print $1, $2}}' {params.prefix_dir}/sites.txt > {params.prefix_dir}/regions.txt

        # Select those sites present in all from the Mutect2 file
        bcftools view -R {params.prefix_dir}/regions.txt -Oz -o {output.sites_vcf} {input.mutect_vcf}
        bcftools index -t {output.sites_vcf}
        """


rule annotate_consensus:
    input:
        cons_vcf = f"{CONSENSUS_PAIRED_OUT}/{{sample}}/{{sample}}.vcf.gz"
    output:
        maf = f"{ANNOTATED_PAIRED_OUT}/{{sample}}.maf"
    log:
        f"{ANNOTATED_PAIRED_OUT}/{{sample}}.log"
    params:
        data_source = config["gatk"]["data_source"],
        ref = config["bwa"]["idx"]
    shell:
        """
        gatk Funcotator \
            --data-sources-path {params.data_source} \
            --output {output.maf} \
            --output-file-format MAF \
            --ref-version hg38 \
            --reference {params.ref} \
            --variant {input.cons_vcf} &> {log}
        """
